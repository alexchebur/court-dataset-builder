# ============================================================================
# –§–ê–ô–õ: app.py
# –ü–†–ò–õ–û–ñ–ï–ù–ò–ï: –°–±–æ—Ä—â–∏–∫ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å—É–¥–µ–±–Ω—ã—Ö –∞–∫—Ç–æ–≤ –¥–ª—è Streamlit Cloud
# –í–ï–†–°–ò–Ø: 2.0 (—Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π JSONL –∏ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è)
# ============================================================================

import streamlit as st
import pdfplumber
import json
import re
import os
import zipfile
import io
from datetime import datetime
from pathlib import Path
import pandas as pd
from typing import List, Dict, Optional

# –ò–º–ø–æ—Ä—Ç —É—Ç–∏–ª–∏—Ç (–µ—Å–ª–∏ –≤—ã–Ω–µ—Å–µ–Ω—ã –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏)
# from utils.pdf_extractor import extract_text_from_pdf, clean_text
# from utils.jsonl_handler import load_jsonl, save_jsonl, merge_datasets
# from utils.data_processor import extract_case_info_from_filename

# ============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –°–¢–†–ê–ù–ò–¶–´
# ============================================================================
st.set_page_config(
    page_title="–°–±–æ—Ä—â–∏–∫ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å—É–¥–µ–±–Ω—ã—Ö –∞–∫—Ç–æ–≤",
    page_icon="‚öñÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================================================
# CSS –°–¢–ò–õ–ò
# ============================================================================
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.2rem;
        color: #666;
        text-align: center;
        margin-bottom: 2rem;
    }
    .stats-box {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    .preview-box {
        background-color: #ffffff;
        border: 1px solid #ddd;
        border-radius: 0.5rem;
        padding: 1rem;
        max-height: 400px;
        overflow-y: auto;
        font-family: monospace;
        font-size: 0.9rem;
    }
    .success-box {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        color: #155724;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
    }
    .error-box {
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
        color: #721c24;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
    }
    .info-box {
        background-color: #d1ecf1;
        border: 1px solid #bee5eb;
        color: #0c5460;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# ============================================================================
# –§–£–ù–ö–¶–ò–ò –û–ë–†–ê–ë–û–¢–ö–ò –î–ê–ù–ù–´–•
# ============================================================================

def clean_text(text: str) -> str:
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –Ω–µ—á–∏—Ç–∞–µ–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ PDF."""
    if not text:
        return ""
    
    if isinstance(text, bytes):
        text = text.decode('utf-8', errors='ignore')
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–ø–µ—á–∞—Ç–∞–µ–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)
    text = re.sub(r'[ \t]+', ' ', text)
    text = re.sub(r'\n\s*\n', '\n\n', text)
    
    lines = text.split('\n')
    lines = [line.strip() for line in lines]
    text = '\n'.join(lines).strip()
    
    # –ó–∞–º–µ–Ω–∞ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
    replacements = {
        'Ô¨Å': '—Ñ–∏', 'Ô¨Ç': '—Ñ–ª', 'Ô¨Ä': '—Ñ—Ñ', 'Ô¨É': '—Ñ—Ñ–∏', 'Ô¨Ñ': '—Ñ—Ñ–ª',
        '‚Äì': '-', '‚Äî': '-', '¬´': '"', '¬ª': '"', '‚Äû': '"', '‚Äö': "'",
        '‚Ä≤': "'", '‚Ä≥': '"', '‚Ä¶': '...', '‚Ä¢': '-', '¬©': '(c)',
        '¬Æ': '(R)', '‚Ñ¢': '(TM)',
    }
    
    for old, new in replacements.items():
        text = text.replace(old, new)
    
    return text


def extract_text_from_pdf(pdf_file) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF-—Ñ–∞–π–ª–∞ —Å –ø–æ–º–æ—â—å—é pdfplumber."""
    text_parts = []
    
    try:
        with pdfplumber.open(pdf_file) as pdf:
            for page_num, page in enumerate(pdf.pages, 1):
                page_text = page.extract_text()
                if page_text:
                    text_parts.append(f"--- –°–¢–†–ê–ù–ò–¶–ê {page_num} ---\n")
                    text_parts.append(page_text)
                    text_parts.append("\n\n")
        
        full_text = ''.join(text_parts)
        return clean_text(full_text)
    
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞: {str(e)}")
        return ""


def extract_case_info_from_filename(filename: str) -> dict:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–º–µ—Ä –¥–µ–ª–∞ –∏ –¥–∞—Ç—É —Ä–µ—à–µ–Ω–∏—è –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞."""
    name_without_ext = Path(filename).stem
    parts = name_without_ext.split('_')
    
    result = {
        'case_number': None,
        'decision_date': None,
        'raw_filename': filename
    }
    
    if len(parts) >= 2:
        result['case_number'] = parts[0]
        date_str = parts[1]
        
        if len(date_str) == 8 and date_str.isdigit():
            try:
                dt = datetime.strptime(date_str, '%Y%m%d')
                result['decision_date'] = dt.strftime('%Y-%m-%d')
            except ValueError:
                result['decision_date'] = date_str
        else:
            result['decision_date'] = date_str
    
    return result


def create_jsonl_entry(case_number: str, decision_date: str, text: str) -> dict:
    """–°–æ–∑–¥–∞—ë—Ç –∑–∞–ø–∏—Å—å —Ñ–æ—Ä–º–∞—Ç–∞ JSONL –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞."""
    return {
        "case_number": case_number,
        "decision_date": decision_date,
        "decision_text": text,
        "metadata": {
            "source": "arbitration_court",
            "document_type": "court_decision",
            "language": "ru",
            "created_at": datetime.now().isoformat()
        }
    }


def create_instruction_dataset_entry(case_number: str, decision_date: str, text: str) -> dict:
    """–°–æ–∑–¥–∞—ë—Ç –∑–∞–ø–∏—Å—å –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞."""
    return {
        "instruction": f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å—É–¥–µ–±–Ω—ã–π –∞–∫—Ç –ø–æ –¥–µ–ª—É ‚Ññ {case_number} –æ—Ç {decision_date}",
        "input": text[:2000],
        "output": f"–°—É–¥–µ–±–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –ø–æ –¥–µ–ª—É {case_number} –æ—Ç {decision_date}. –¢–µ–∫—Å—Ç —Ä–µ—à–µ–Ω–∏—è: {text[:3000]}..."
    }


# ============================================================================
# –†–ê–ë–û–¢–ê –° JSONL (–ó–ê–ì–†–£–ó–ö–ê / –°–û–•–†–ê–ù–ï–ù–ò–ï / –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï)
# ============================================================================

def load_jsonl_dataset(uploaded_file) -> List[dict]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ JSONL —Ñ–∞–π–ª–∞."""
    entries = []
    try:
        content = uploaded_file.read().decode('utf-8')
        for line in content.strip().split('\n'):
            if line.strip():
                entries.append(json.loads(line))
        return entries
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ JSONL: {e}")
        return []


def save_jsonl_dataset(entries: List[dict]) -> str:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSONL –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É."""
    return '\n'.join(json.dumps(entry, ensure_ascii=False) for entry in entries)


def merge_datasets(existing: List[dict], new: List[dict]) -> List[dict]:
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞, –∏–∑–±–µ–≥–∞—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ –Ω–æ–º–µ—Ä—É –¥–µ–ª–∞."""
    existing_cases = {e.get('case_number') for e in existing if e.get('case_number')}
    merged = existing.copy()
    
    for entry in new:
        if entry.get('case_number') not in existing_cases:
            merged.append(entry)
            existing_cases.add(entry.get('case_number'))
    
    return merged


def create_download_package(entries: List[dict], instruction_entries: List[dict] = None) -> bytes:
    """–°–æ–∑–¥–∞—ë—Ç ZIP-–∞—Ä—Ö–∏–≤ —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è."""
    zip_buffer = io.BytesIO()
    
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
        # –û—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç
        jsonl_content = save_jsonl_dataset(entries)
        zip_file.writestr('court_decisions_dataset.jsonl', jsonl_content.encode('utf-8'))
        
        # –ò–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        if instruction_entries:
            instr_content = save_jsonl_dataset(instruction_entries)
            zip_file.writestr('instruction_dataset.jsonl', instr_content.encode('utf-8'))
        
        # README
        readme_content = f"""# –î–∞—Ç–∞—Å–µ—Ç —Å—É–¥–µ–±–Ω—ã—Ö –∞–∫—Ç–æ–≤ –∞—Ä–±–∏—Ç—Ä–∞–∂–Ω—ã—Ö —Å—É–¥–æ–≤

## –û–ø–∏—Å–∞–Ω–∏–µ
–î–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç—ã —Å—É–¥–µ–±–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –∞—Ä–±–∏—Ç—Ä–∞–∂–Ω—ã—Ö —Å—É–¥–æ–≤ –†–æ—Å—Å–∏–∏.

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–æ–≤
- `court_decisions_dataset.jsonl` - –û—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç
- `instruction_dataset.jsonl` - –ò–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è Fine-tuning

## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
- –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(entries)}
- –î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ
- –û–±—É—á–µ–Ω–∏–µ LoRA-–∞–¥–∞–ø—Ç–µ—Ä–æ–≤ –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö LLM
- Fine-tuning –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—É–¥–µ–±–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π
"""
        zip_file.writestr('README.md', readme_content.encode('utf-8'))
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ CSV
        if entries:
            df = pd.DataFrame([{
                'case_number': e.get('case_number', ''),
                'decision_date': e.get('decision_date', ''),
                'text_length': len(e.get('decision_text', ''))
            } for e in entries])
            csv_content = df.to_csv(index=False, encoding='utf-8-sig')
            zip_file.writestr('dataset_statistics.csv', csv_content.encode('utf-8'))
    
    zip_buffer.seek(0)
    return zip_buffer.getvalue()


# ============================================================================
# SESSION STATE –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø
# ============================================================================
if 'dataset_entries' not in st.session_state:
    st.session_state.dataset_entries = []
if 'processed_files' not in st.session_state:
    st.session_state.processed_files = set()
if 'last_updated' not in st.session_state:
    st.session_state.last_updated = None

# ============================================================================
# –ó–ê–ì–û–õ–û–í–û–ö
# ============================================================================
st.markdown('<h1 class="main-header">‚öñÔ∏è –°–±–æ—Ä—â–∏–∫ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å—É–¥–µ–±–Ω—ã—Ö –∞–∫—Ç–æ–≤</h1>', 
            unsafe_allow_html=True)
st.markdown('<p class="sub-header">–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LoRA –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ | Streamlit Cloud</p>', 
            unsafe_allow_html=True)

# ============================================================================
# –ë–û–ö–û–í–ê–Ø –ü–ê–ù–ï–õ–¨
# ============================================================================
with st.sidebar:
    st.header("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞")
    
    st.metric("–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π", len(st.session_state.dataset_entries))
    st.metric("–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤", len(st.session_state.processed_files))
    
    if st.session_state.dataset_entries:
        dates = [e.get('decision_date', '') for e in st.session_state.dataset_entries if e.get('decision_date')]
        if dates:
            st.metric("–î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç", f"{min(dates)} ‚Äî {max(dates)}")
        
        total_chars = sum(len(e.get('decision_text', '')) for e in st.session_state.dataset_entries)
        st.metric("–û–±—â–∏–π –æ–±—ä—ë–º —Ç–µ–∫—Å—Ç–∞", f"{total_chars:,} —Å–∏–º–≤–æ–ª–æ–≤")
    
    st.divider()
    
    st.subheader("üõ† –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")
    
    if st.button("üóë –û—á–∏—Å—Ç–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç", use_container_width=True):
        st.session_state.dataset_entries = []
        st.session_state.processed_files = set()
        st.session_state.last_updated = None
        st.rerun()
    
    st.divider()
    
    st.subheader("‚ÑπÔ∏è –û –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏")
    st.info("""
    **–í–µ—Ä—Å–∏—è:** 2.0  
    **–§–æ—Ä–º–∞—Ç:** JSONL  
    **–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** LoRA, Fine-tuning  
    **–î–µ–ø–ª–æ–π:** Streamlit Cloud + GitHub
    """)

# ============================================================================
# –û–°–ù–û–í–ù–ê–Ø –û–ë–õ–ê–°–¢–¨ - –ó–ê–ì–†–£–ó–ö–ê JSONL
# ============================================================================
st.header("üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ (JSONL)")

col1, col2 = st.columns([2, 1])

with col1:
    uploaded_jsonl = st.file_uploader(
        "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSONL –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã",
        type=['jsonl', 'json'],
        help="–≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å–±–æ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å —Ç–æ–≥–æ –º–µ—Å—Ç–∞, –≥–¥–µ –≤—ã –æ—Å—Ç–∞–Ω–æ–≤–∏–ª–∏—Å—å"
    )

with col2:
    if uploaded_jsonl:
        if st.button("üìÇ –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç", use_container_width=True):
            entries = load_jsonl_dataset(uploaded_jsonl)
            if entries:
                st.session_state.dataset_entries = merge_datasets(
                    st.session_state.dataset_entries,
                    entries
                )
                st.session_state.last_updated = datetime.now().isoformat()
                st.success(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(entries)}")
                st.rerun()

if st.session_state.dataset_entries:
    st.success(f"‚úÖ –í —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏: {len(st.session_state.dataset_entries)} –∑–∞–ø–∏—Å–µ–π")

# ============================================================================
# –ó–ê–ì–†–£–ó–ö–ê PDF –§–ê–ô–õ–û–í
# ============================================================================
st.divider()
st.header("üì§ –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤—ã—Ö —Å—É–¥–µ–±–Ω—ã—Ö –∞–∫—Ç–æ–≤ (PDF)")

uploaded_files = st.file_uploader(
    "–ó–∞–≥—Ä—É–∑–∏—Ç–µ PDF-—Ñ–∞–π–ª—ã —Å—É–¥–µ–±–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π",
    type=['pdf'],
    accept_multiple_files=True,
    help="–ú–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ"
)

if uploaded_files:
    st.write(f"–í—ã–±—Ä–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: **{len(uploaded_files)}**")
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    new_entries = []
    
    for idx, uploaded_file in enumerate(uploaded_files):
        if uploaded_file.name in st.session_state.processed_files:
            continue
        
        status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ {idx + 1}/{len(uploaded_files)}: {uploaded_file.name}")
        
        try:
            file_info = extract_case_info_from_filename(uploaded_file.name)
            
            if not file_info['case_number'] or not file_info['decision_date']:
                st.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –∏–º–µ–Ω–∏: {uploaded_file.name}")
                continue
            
            pdf_text = extract_text_from_pdf(uploaded_file)
            
            if not pdf_text or len(pdf_text) < 100:
                st.warning(f"‚ö†Ô∏è –¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π: {uploaded_file.name}")
                continue
            
            jsonl_entry = create_jsonl_entry(
                file_info['case_number'],
                file_info['decision_date'],
                pdf_text
            )
            
            new_entries.append(jsonl_entry)
            st.session_state.dataset_entries.append(jsonl_entry)
            st.session_state.processed_files.add(uploaded_file.name)
            
            progress_bar.progress((idx + 1) / len(uploaded_files))
            
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {uploaded_file.name}: {str(e)}")
            continue
    
    status_text.text("‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    progress_bar.empty()
    
    if new_entries:
        st.success(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(new_entries)}")
        st.session_state.last_updated = datetime.now().isoformat()

# ============================================================================
# –ü–†–ï–î–ü–†–û–°–ú–û–¢–† –î–ê–¢–ê–°–ï–¢–ê
# ============================================================================
if st.session_state.dataset_entries:
    st.divider()
    st.header("üëÅ –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞")
    
    selected_idx = st.selectbox(
        "–í—ã–±–µ—Ä–∏—Ç–µ –∑–∞–ø–∏—Å—å –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞",
        range(len(st.session_state.dataset_entries)),
        format_func=lambda x: f"{st.session_state.dataset_entries[x]['case_number']} –æ—Ç {st.session_state.dataset_entries[x]['decision_date']}"
    )
    
    if selected_idx is not None:
        entry = st.session_state.dataset_entries[selected_idx]
        
        col1, col2 = st.columns([1, 2])
        
        with col1:
            st.subheader("üìã –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ")
            st.json({
                "–ù–æ–º–µ—Ä –¥–µ–ª–∞": entry['case_number'],
                "–î–∞—Ç–∞ —Ä–µ—à–µ–Ω–∏—è": entry['decision_date'],
                "–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞": len(entry['decision_text']),
                "–°–æ–∑–¥–∞–Ω–æ": entry['metadata']['created_at'][:19]
            })
        
        with col2:
            st.subheader("üìÑ –¢–µ–∫—Å—Ç —Ä–µ—à–µ–Ω–∏—è (—Ñ—Ä–∞–≥–º–µ–Ω—Ç)")
            preview_text = entry['decision_text'][:2000]
            st.text_area(
                "–°–æ–¥–µ—Ä–∂–∏–º–æ–µ",
                value=preview_text,
                height=400,
                label_visibility="collapsed"
            )
    
    # –¢–∞–±–ª–∏—Ü–∞ –≤—Å–µ—Ö –∑–∞–ø–∏—Å–µ–π
    st.subheader("üìä –í—Å–µ –∑–∞–ø–∏—Å–∏ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ")
    
    df_display = pd.DataFrame([
        {
            "‚Ññ": idx + 1,
            "–ù–æ–º–µ—Ä –¥–µ–ª–∞": e['case_number'],
            "–î–∞—Ç–∞ —Ä–µ—à–µ–Ω–∏—è": e['decision_date'],
            "–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞": len(e['decision_text']),
            "–°—Ç–∞—Ç—É—Å": "‚úÖ"
        }
        for idx, e in enumerate(st.session_state.dataset_entries)
    ])
    
    st.dataframe(df_display, use_container_width=True, hide_index=True)

# ============================================================================
# –≠–ö–°–ü–û–†–¢ –î–ê–¢–ê–°–ï–¢–ê
# ============================================================================
if st.session_state.dataset_entries:
    st.divider()
    st.header("üíæ –≠–∫—Å–ø–æ—Ä—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        jsonl_content = save_jsonl_dataset(st.session_state.dataset_entries)
        
        st.download_button(
            label="üì• –°–∫–∞—á–∞—Ç—å JSONL",
            data=jsonl_content.encode('utf-8'),
            file_name=f"court_decisions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl",
            mime="text/jsonl",
            use_container_width=True
        )
    
    with col2:
        instruction_entries = [
            create_instruction_dataset_entry(
                e['case_number'], 
                e['decision_date'], 
                e['decision_text']
            )
            for e in st.session_state.dataset_entries
        ]
        
        instr_content = save_jsonl_dataset(instruction_entries)
        
        st.download_button(
            label="üì• –°–∫–∞—á–∞—Ç—å Instruction Dataset",
            data=instr_content.encode('utf-8'),
            file_name=f"instruction_dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl",
            mime="text/jsonl",
            use_container_width=True
        )
    
    with col3:
        zip_data = create_download_package(
            st.session_state.dataset_entries,
            instruction_entries
        )
        
        st.download_button(
            label="üì¶ –°–∫–∞—á–∞—Ç—å ZIP-–∞—Ä—Ö–∏–≤",
            data=zip_data,
            file_name=f"court_dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
            mime="application/zip",
            use_container_width=True
        )
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–æ—Ä–º–∞—Ç–∞—Ö
    with st.expander("‚ÑπÔ∏è –û–ø–∏—Å–∞–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–æ–≤ —ç–∫—Å–ø–æ—Ä—Ç–∞"):
        st.markdown("""
        ### üìÑ JSONL (–û—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç)
        - –§–æ—Ä–º–∞—Ç: JSON Lines (–æ–¥–Ω–∞ JSON-–∑–∞–ø–∏—Å—å –Ω–∞ —Å—Ç—Ä–æ–∫—É)
        - –ü–æ–ª—è: `case_number`, `decision_date`, `decision_text`, `metadata`
        - –ö–æ–¥–∏—Ä–æ–≤–∫–∞: UTF-8
        
        ### üìö Instruction Dataset
        - –§–æ—Ä–º–∞—Ç: JSON Lines —Å –ø–æ–ª—è–º–∏ `instruction`, `input`, `output`
        - –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: Fine-tuning LLM (LoRA, QLoRA)
        
        ### üì¶ ZIP-–∞—Ä—Ö–∏–≤
        - –í–∫–ª—é—á–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç, –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏ README
        """)

# ============================================================================
# –ü–û–î–í–ê–õ
# ============================================================================
st.divider()
st.markdown("""
<div style="text-align: center; color: #666; font-size: 0.9rem;">
    <p>‚öñÔ∏è –°–±–æ—Ä—â–∏–∫ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å—É–¥–µ–±–Ω—ã—Ö –∞–∫—Ç–æ–≤ | –í–µ—Ä—Å–∏—è 2.0 | Streamlit Cloud</p>
    <p>–î–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω—ã –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —Ü–µ–ª–µ–π</p>
    <p>–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: {last_updated}</p>
</div>
""".format(last_updated=st.session_state.last_updated or "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"), unsafe_allow_html=True)
